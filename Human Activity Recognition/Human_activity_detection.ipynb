{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:right\"><span style=\"color:black; font-family:Georgia; font-size:1em\">Document Created date:19-01-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><span style=\"color:green; font-family:Georgia; font-size:3em;\">Human Activity Recognition</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='har.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left\"><span style=\"color:green; font-family:Georgia; font-size:2em;\">Understanding Data</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "This project is to build a model that predicts the human activities such as Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing or Laying.\n",
    "\n",
    "This dataset is collected from 30 persons(referred as subjects in this dataset), performing different activities with a smartphone to their waists. The data is recorded with the help of sensors (accelerometer and Gyroscope) in that smartphone. This experiment was video recorded to label the data manually.\n",
    "\n",
    "## How data was recorded\n",
    "\n",
    "By using the sensors(Gyroscope and accelerometer) in a smartphone, they have captured '3-axial linear acceleration'(_tAcc-XYZ_) from accelerometer and '3-axial angular velocity' (_tGyro-XYZ_) from Gyroscope with several variations. \n",
    "\n",
    "> prefix 't' in those metrics denotes time.\n",
    "\n",
    "> suffix 'XYZ' represents 3-axial signals in X , Y, and Z directions.\n",
    "\n",
    "\n",
    "###  Y_Labels(Encoded)\n",
    "+ In the dataset, Y_labels are represented as numbers from 1 to 6 as their identifiers.\n",
    "\n",
    "\t- WALKING as __1__\n",
    "\t- WALKING_UPSTAIRS as __2__\n",
    "\t- WALKING_DOWNSTAIRS as __3__\n",
    "\t- SITTING as __4__\n",
    "\t- STANDING as __5__\n",
    "\t- LAYING as __6__\n",
    "    \n",
    "## Train and test data were saperated\n",
    " - The readings from ___70%___ of the volunteers were taken as ___trianing data___ and remaining ___30%___ subjects recordings were taken for ___test data___\n",
    " \n",
    "## Data\n",
    "\n",
    "* All the data is present in 'UCI_HAR_dataset/' folder in present working directory.\n",
    "     - Feature names are present in 'UCI_HAR_dataset/features.txt'\n",
    "     - ___Train Data___\n",
    "         - 'UCI_HAR_dataset/train/X_train.txt'\n",
    "         - 'UCI_HAR_dataset/train/subject_train.txt'\n",
    "         - 'UCI_HAR_dataset/train/y_train.txt'\n",
    "     - ___Test Data___\n",
    "         - 'UCI_HAR_dataset/test/X_test.txt'\n",
    "         - 'UCI_HAR_dataset/test/subject_test.txt'\n",
    "         - 'UCI_HAR_dataset/test/y_test.txt'\n",
    "         \n",
    "\n",
    "## Data Size :\n",
    "> 27 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Accelerometer and Gyroscope readings are taken from 30 volunteers(referred as subjects) while performing the following 6 Activities.\n",
    "\n",
    "    1. Walking     \n",
    "    2. WalkingUpstairs \n",
    "    3. WalkingDownstairs \n",
    "    4. Standing \n",
    "    5. Sitting \n",
    "    6. Lying.\n",
    "\n",
    "\n",
    "* Readings are divided into a window of 2.56 seconds with 50% overlapping. \n",
    "\n",
    "* Accelerometer readings are divided into gravity acceleration and body acceleration readings,\n",
    "  which has x,y and z components each.\n",
    "\n",
    "* Gyroscope readings are the measure of angular velocities which has x,y and z components.\n",
    "\n",
    "* Jerk signals are calculated for BodyAcceleration readings.\n",
    "\n",
    "* Fourier Transforms are made on the above time readings to obtain frequency readings.\n",
    "\n",
    "* Now, on all the base signal readings., mean, max, mad, sma, arcoefficient, engerybands,entropy etc., are calculated for each window.\n",
    "\n",
    "* We get a feature vector of 561 features and these features are given in the dataset.\n",
    "\n",
    "* Each window of readings is a datapoint of 561 features.\n",
    "\n",
    "## Problem Framework\n",
    "\n",
    "* 30 subjects(volunteers) data is randomly split to 70%(21) test and 30%(7) train data.\n",
    "* Each datapoint corresponds one of the 6 Activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    " + Given a new datapoint we have to predict the Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise : HAR_LSTM HYPERPARATEMER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "pIaPf3GZbIAl",
    "outputId": "6aa3d2f2-0164-43fe-c15f-d7a5fb2c6365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mounting Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37DNs4d3bMDd"
   },
   "outputs": [],
   "source": [
    "# Unzip the Data\n",
    "import zipfile\n",
    "zipfile_from = zipfile.ZipFile('/content/drive/My Drive/HumanActivityRecognition.zip','r')\n",
    "zipfile_from.extractall('/content/drive/My Drive/HumanActivityrecognition')\n",
    "zipfile_from.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTDXjK5Xa7z5"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cumqI47Fa7z-"
   },
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwVU6nova70D"
   },
   "source": [
    "### Data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovNsurLTa70E"
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSl4iFDUa70J"
   },
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUlh-Bjda70N"
   },
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'/content/drive/My Drive/HumanActivityrecognition/HAR/UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).as_matrix()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtlx9H5ra70R"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'/content/drive/My Drive/HumanActivityrecognition/HAR/UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUIv0G4za70V"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pqW9Zx3a70a"
   },
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGIEMKFXa70g"
   },
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ed3fFNBna70k"
   },
   "outputs": [],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0-UVpCLa70o"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8B_pFUc3a70t"
   },
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "n_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhJ2S4vja70x"
   },
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "hf9rWPGya702",
    "outputId": "c81c1463-ed23-4e21-a546-fbf54e86b261"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "qToCMMvDa706",
    "outputId": "6836ee0a-db44-41d0-b13d-59d9bb6f4a19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpzQ0uDea70-"
   },
   "source": [
    "### Model :: Defining the Architecture of LSTM(32) + Dropout(0.5) + rmsprop + sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "IT3J2GKra71A",
    "outputId": "9031d0d3-1248-4960-df24-ad03e0508ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 32)                5376      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 5,574\n",
      "Trainable params: 5,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "jDtfe1xka71E",
    "outputId": "4ceedaf2-ae29-4b2f-e94b-bc7377c4ed1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JGfkMSgOa71I",
    "outputId": "85b359a5-cd35-460f-baeb-d66c21e4f637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 1.3117 - acc: 0.4410 - val_loss: 1.1254 - val_acc: 0.4730\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.9697 - acc: 0.5919 - val_loss: 0.8992 - val_acc: 0.5931\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.7834 - acc: 0.6532 - val_loss: 0.7576 - val_acc: 0.6159\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.6817 - acc: 0.6678 - val_loss: 0.6802 - val_acc: 0.6210\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.6346 - acc: 0.6902 - val_loss: 0.6976 - val_acc: 0.6722\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.6127 - acc: 0.7087 - val_loss: 0.7093 - val_acc: 0.7041\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.5647 - acc: 0.7561 - val_loss: 0.6837 - val_acc: 0.7475\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.5169 - acc: 0.7837 - val_loss: 0.6575 - val_acc: 0.7370\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.4744 - acc: 0.7953 - val_loss: 0.6772 - val_acc: 0.7435\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.4525 - acc: 0.8051 - val_loss: 0.6664 - val_acc: 0.7075\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.4324 - acc: 0.8275 - val_loss: 0.6367 - val_acc: 0.7900\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.3932 - acc: 0.8520 - val_loss: 0.5087 - val_acc: 0.8656\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.3484 - acc: 0.8913 - val_loss: 0.4076 - val_acc: 0.8799\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.3086 - acc: 0.9116 - val_loss: 0.4920 - val_acc: 0.8680\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2776 - acc: 0.9252 - val_loss: 0.9069 - val_acc: 0.7998\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2680 - acc: 0.9229 - val_loss: 0.4340 - val_acc: 0.8724\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2403 - acc: 0.9300 - val_loss: 0.4533 - val_acc: 0.8894\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2587 - acc: 0.9236 - val_loss: 0.4504 - val_acc: 0.8968\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1937 - acc: 0.9418 - val_loss: 0.5122 - val_acc: 0.8795\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1992 - acc: 0.9378 - val_loss: 0.4035 - val_acc: 0.8931\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1899 - acc: 0.9402 - val_loss: 0.4522 - val_acc: 0.8958\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2051 - acc: 0.9382 - val_loss: 0.5021 - val_acc: 0.8894\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1791 - acc: 0.9404 - val_loss: 0.4781 - val_acc: 0.8951\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1746 - acc: 0.9456 - val_loss: 0.4376 - val_acc: 0.8989\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1961 - acc: 0.9384 - val_loss: 0.4530 - val_acc: 0.8880\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1802 - acc: 0.9452 - val_loss: 0.4403 - val_acc: 0.8951\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1604 - acc: 0.9452 - val_loss: 0.4152 - val_acc: 0.8989\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1552 - acc: 0.9455 - val_loss: 0.4937 - val_acc: 0.8948\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1627 - acc: 0.9472 - val_loss: 0.5597 - val_acc: 0.8904\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1748 - acc: 0.9433 - val_loss: 0.5089 - val_acc: 0.8958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f182097bf28>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "mbzcUbVsa71P",
    "outputId": "67a4115c-dc1c-4355-bffb-d26f1dc377c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 510        0  ...                   0                26\n",
      "SITTING                  0      376  ...                   2                 1\n",
      "STANDING                 0       82  ...                   0                 1\n",
      "WALKING                  0        0  ...                  28                 5\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 412                 8\n",
      "WALKING_UPSTAIRS         0        0  ...                  24               430\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xUT5HGVea71Y",
    "outputId": "a18e4165-bd61-4bbd-8b29-7128c355efd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 1s 367us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oqQIjI8ra71e",
    "outputId": "ce45b872-5bfb-41a5-c0d5-9ee2d0d237c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5088777291587507, 0.8958262639972854]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQBE42lka71i"
   },
   "source": [
    "- With a simple 2 layer architecture we got 90.09% accuracy and a loss of 0.30\n",
    "- We can further imporve the performace with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vk4fneg5qMbO"
   },
   "source": [
    "### Model1:: Architecture of LSTM(32) + Dropout(0.5) + rmsprop + sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c5DWpuuPfvE5",
    "outputId": "8284a9ce-f636-417d-f754-854282d42c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 32)                5376      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 5,574\n",
      "Trainable params: 5,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 1.3362 - acc: 0.4445 - val_loss: 1.2370 - val_acc: 0.5131\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 1.0587 - acc: 0.5453 - val_loss: 1.0204 - val_acc: 0.5080\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.8865 - acc: 0.5990 - val_loss: 1.0198 - val_acc: 0.5860\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.8135 - acc: 0.6125 - val_loss: 0.9102 - val_acc: 0.6033\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.7604 - acc: 0.6496 - val_loss: 0.8154 - val_acc: 0.6393\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.7038 - acc: 0.6790 - val_loss: 0.7573 - val_acc: 0.7170\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.6713 - acc: 0.7021 - val_loss: 0.8712 - val_acc: 0.6610\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.6434 - acc: 0.7469 - val_loss: 0.8214 - val_acc: 0.7340\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.5610 - acc: 0.7980 - val_loss: 0.7142 - val_acc: 0.7553\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.4465 - acc: 0.8572 - val_loss: 0.6353 - val_acc: 0.8144\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.3947 - acc: 0.8799 - val_loss: 0.6083 - val_acc: 0.8090\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.3477 - acc: 0.8965 - val_loss: 0.6552 - val_acc: 0.8232\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2979 - acc: 0.9093 - val_loss: 0.4861 - val_acc: 0.8666\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.3046 - acc: 0.9087 - val_loss: 0.5533 - val_acc: 0.8510\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2699 - acc: 0.9180 - val_loss: 0.4373 - val_acc: 0.8860\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2486 - acc: 0.9253 - val_loss: 0.4164 - val_acc: 0.8850\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2335 - acc: 0.9286 - val_loss: 0.7569 - val_acc: 0.8219\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2339 - acc: 0.9266 - val_loss: 0.4528 - val_acc: 0.8744\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2290 - acc: 0.9261 - val_loss: 0.3914 - val_acc: 0.8924\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.2229 - acc: 0.9304 - val_loss: 0.4858 - val_acc: 0.8823\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2012 - acc: 0.9329 - val_loss: 0.3812 - val_acc: 0.8914\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1896 - acc: 0.9373 - val_loss: 0.4334 - val_acc: 0.8911\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1898 - acc: 0.9377 - val_loss: 0.4008 - val_acc: 0.8887\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1890 - acc: 0.9391 - val_loss: 0.4503 - val_acc: 0.8724\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1855 - acc: 0.9396 - val_loss: 0.3993 - val_acc: 0.8918\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1810 - acc: 0.9421 - val_loss: 0.3557 - val_acc: 0.9002\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2109 - acc: 0.9361 - val_loss: 0.5865 - val_acc: 0.8924\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1622 - acc: 0.9446 - val_loss: 0.3609 - val_acc: 0.9002\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1613 - acc: 0.9430 - val_loss: 0.4424 - val_acc: 0.8985\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1648 - acc: 0.9489 - val_loss: 0.3348 - val_acc: 0.9155\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model1 = Sequential()\n",
    "# Configuring the parameters\n",
    "model1.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model1.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model1.add(Dense(n_classes, activation='sigmoid'))\n",
    "model1.summary()\n",
    "\n",
    "# Compiling the model\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history1 = model1.fit(X_train, Y_train, batch_size=batch_size,validation_data=(X_test, Y_test),epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "5xuRpTlfpC6v",
    "outputId": "4926367f-f06d-41b1-8bb8-11062d0cd247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 537        0  ...                   0                 0\n",
      "SITTING                  0      380  ...                   0                 3\n",
      "STANDING                 0       59  ...                   0                 0\n",
      "WALKING                  0        0  ...                  14                14\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 413                 3\n",
      "WALKING_UPSTAIRS         0        0  ...                  16               434\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2VVFEV1upOs-",
    "outputId": "cd3cd9d1-dd89-4979-d613-261126feef1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 1s 383us/step\n"
     ]
    }
   ],
   "source": [
    "score1 = model1.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qjVbdDZcpOpp",
    "outputId": "a62f4dfd-0457-4575-8fcc-9afe577c4d10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33480536445324116, 0.9155072955548015]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3lvDaS6qqEBA"
   },
   "source": [
    "### Model2:: Architecture of LSTM(42) + Dropout(0.5) + rmsprop + sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ensVFUQWpOm1",
    "outputId": "4c32c231-800f-4fa1-bd36-032cc806fd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 42)                8736      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 42)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 258       \n",
      "=================================================================\n",
      "Total params: 8,994\n",
      "Trainable params: 8,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.3137 - acc: 0.4222 - val_loss: 1.2687 - val_acc: 0.4136\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 1.0355 - acc: 0.5498 - val_loss: 1.0039 - val_acc: 0.5416\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.9231 - acc: 0.6034 - val_loss: 0.8401 - val_acc: 0.6345\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.7992 - acc: 0.6453 - val_loss: 0.9690 - val_acc: 0.6495\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.6993 - acc: 0.7014 - val_loss: 0.8239 - val_acc: 0.6922\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.6378 - acc: 0.7333 - val_loss: 0.7043 - val_acc: 0.7509\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.5463 - acc: 0.7994 - val_loss: 0.5985 - val_acc: 0.7974\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.4979 - acc: 0.8319 - val_loss: 0.6876 - val_acc: 0.7822\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.5142 - acc: 0.8229 - val_loss: 0.6982 - val_acc: 0.7828\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.4176 - acc: 0.8653 - val_loss: 0.4709 - val_acc: 0.8354\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.3551 - acc: 0.8925 - val_loss: 0.5235 - val_acc: 0.8354\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2898 - acc: 0.9154 - val_loss: 0.4995 - val_acc: 0.8588\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.4432 - acc: 0.8730 - val_loss: 0.4778 - val_acc: 0.8303\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2852 - acc: 0.9117 - val_loss: 0.4824 - val_acc: 0.8717\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2360 - acc: 0.9279 - val_loss: 0.4126 - val_acc: 0.8778\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.2132 - acc: 0.9359 - val_loss: 0.4838 - val_acc: 0.8649\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2156 - acc: 0.9347 - val_loss: 0.5602 - val_acc: 0.8605\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1876 - acc: 0.9399 - val_loss: 0.4132 - val_acc: 0.8826\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1852 - acc: 0.9377 - val_loss: 0.3586 - val_acc: 0.8877\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2028 - acc: 0.9389 - val_loss: 0.3264 - val_acc: 0.9016\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1925 - acc: 0.9418 - val_loss: 0.4438 - val_acc: 0.8989\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1725 - acc: 0.9437 - val_loss: 0.2788 - val_acc: 0.9087\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1867 - acc: 0.9355 - val_loss: 0.2854 - val_acc: 0.9006\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1755 - acc: 0.9391 - val_loss: 0.2862 - val_acc: 0.9135\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1634 - acc: 0.9427 - val_loss: 0.4857 - val_acc: 0.8843\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1557 - acc: 0.9446 - val_loss: 0.3935 - val_acc: 0.8945\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1572 - acc: 0.9498 - val_loss: 0.4415 - val_acc: 0.8945\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 0.1680 - acc: 0.9464 - val_loss: 0.4496 - val_acc: 0.8996\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1592 - acc: 0.9463 - val_loss: 0.4654 - val_acc: 0.8860\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1833 - acc: 0.9437 - val_loss: 0.8907 - val_acc: 0.8493\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model2 = Sequential()\n",
    "# Configuring the parameters\n",
    "model2.add(LSTM(42, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model2.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model2.add(Dense(n_classes, activation='sigmoid'))\n",
    "print(model2.summary())\n",
    "\n",
    "# Compiling the model\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history2 = model2.fit(X_train,Y_train,batch_size=batch_size,validation_data=(X_test, Y_test),epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jKxGzQJ-ql9D",
    "outputId": "43939b1c-6c21-4baf-8e8a-8ac307793b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 1s 432us/step\n"
     ]
    }
   ],
   "source": [
    "score2 = model2.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1LBdqYhNqmLv",
    "outputId": "e964f0e8-c755-439a-ec3d-1ba58e5acff9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8907021589124672, 0.8493383101459111]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bs46y2KnqoXb"
   },
   "source": [
    "### Model3:: Architecture of LSTM(80) + Dropout(0.25) + adam + sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "pwibAO0eqpDE",
    "outputId": "c1e1e10f-eec5-4bfa-d065-73d962a5166b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 80)                28800     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 486       \n",
      "=================================================================\n",
      "Total params: 29,286\n",
      "Trainable params: 29,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# With One LSTM Layer Model 1  # \n",
    "n_hidden = 80\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "# 1 LSTM layer\n",
    "model3.add(LSTM(n_hidden, input_shape = (timesteps, input_dim)))     # 1 LSTM\n",
    "\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Dense(n_classes, activation = 'sigmoid'))\n",
    "model3.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z4-2yL-6qr7o",
    "outputId": "1b83b8f1-8db5-4b6c-dacf-118ad78012b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.4040 - acc: 0.8510 - val_loss: 0.3652 - val_acc: 0.8564\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.3532 - acc: 0.8625 - val_loss: 0.3597 - val_acc: 0.8598\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.3647 - acc: 0.8567 - val_loss: 0.3311 - val_acc: 0.8608\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.3385 - acc: 0.8587 - val_loss: 0.3191 - val_acc: 0.8702\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.3059 - acc: 0.8739 - val_loss: 0.2738 - val_acc: 0.8756\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.2742 - acc: 0.8867 - val_loss: 0.2484 - val_acc: 0.8894\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.2912 - acc: 0.8795 - val_loss: 0.2946 - val_acc: 0.8738\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.2614 - acc: 0.8816 - val_loss: 0.2477 - val_acc: 0.8879\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.2149 - acc: 0.8987 - val_loss: 0.2236 - val_acc: 0.8950\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.1992 - acc: 0.9033 - val_loss: 0.2206 - val_acc: 0.8931\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.1886 - acc: 0.9091 - val_loss: 0.2382 - val_acc: 0.8794\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.2300 - acc: 0.8969 - val_loss: 0.2463 - val_acc: 0.8854\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.1996 - acc: 0.9064 - val_loss: 0.2107 - val_acc: 0.9119\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.1500 - acc: 0.9381 - val_loss: 0.1855 - val_acc: 0.9243\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.1398 - acc: 0.9454 - val_loss: 0.1801 - val_acc: 0.9334\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.1314 - acc: 0.9518 - val_loss: 0.1750 - val_acc: 0.9311\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.1084 - acc: 0.9617 - val_loss: 0.1380 - val_acc: 0.9495\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.0795 - acc: 0.9739 - val_loss: 0.1206 - val_acc: 0.9585\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.1009 - acc: 0.9634 - val_loss: 0.1540 - val_acc: 0.9527\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.2128 - acc: 0.9246 - val_loss: 0.1639 - val_acc: 0.9453\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.1458 - acc: 0.9471 - val_loss: 0.1596 - val_acc: 0.9494\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.1169 - acc: 0.9602 - val_loss: 0.1271 - val_acc: 0.9566\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.0853 - acc: 0.9730 - val_loss: 0.1216 - val_acc: 0.9591\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.0954 - acc: 0.9663 - val_loss: 0.1185 - val_acc: 0.9603\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.0889 - acc: 0.9685 - val_loss: 0.1252 - val_acc: 0.9544\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.0800 - acc: 0.9722 - val_loss: 0.1241 - val_acc: 0.9607\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.0635 - acc: 0.9782 - val_loss: 0.1190 - val_acc: 0.9640\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.0670 - acc: 0.9772 - val_loss: 0.1224 - val_acc: 0.9632\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.0575 - acc: 0.9796 - val_loss: 0.1246 - val_acc: 0.9597\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 0.0655 - acc: 0.9752 - val_loss: 0.1164 - val_acc: 0.9629\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history3 = model3.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=64,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0_iGs_KwqsCp",
    "outputId": "ac05ca05-c25d-490e-bdb8-2fb4ee4a541a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 2s 570us/step\n",
      "[0.5088777291587507, 0.8958262639972854]\n"
     ]
    }
   ],
   "source": [
    "score3 = model3.evaluate(X_test, Y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uNFLmQ5hqsAd",
    "outputId": "70bcb96d-c6e0-40c8-a8c5-560400333791"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11640074694648317, 0.9629001287350948]"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "x4RFPA5WrrVp",
    "outputId": "0c4fab56-c643-46a2-e049-e9fee4375ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 510        0  ...                   0                27\n",
      "SITTING                  0      371  ...                   2                 7\n",
      "STANDING                 0       81  ...                   0                12\n",
      "WALKING                  0        0  ...                   6                 4\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 416                 1\n",
      "WALKING_UPSTAIRS         0        0  ...                   9               405\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model3.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKBDa7izrSUG"
   },
   "source": [
    "### Model4 :: Architecture of LSTM(80) + Dropout(0.25) + rmsprop + sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "WHnXRcbxrN2_",
    "outputId": "c0e086a8-eee5-47e4-ba6b-d753cfa4bd3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 128, 80)           28800     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128, 80)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 80)                51520     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 486       \n",
      "=================================================================\n",
      "Total params: 80,806\n",
      "Trainable params: 80,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# With One LSTM Layer Model 1  # \n",
    "n_hidden = 80\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "# 1 LSTM layer\n",
    "model4.add(LSTM(n_hidden, input_shape = (timesteps, input_dim), return_sequences = True))     # 1 LSTM\n",
    "\n",
    "model4.add(Dropout(0.25))\n",
    "model4.add(LSTM(n_hidden)) \n",
    "model4.add(Dense(n_classes, activation = 'sigmoid'))\n",
    "\n",
    "model4.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "print(model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pOR2NUr3rUYw",
    "outputId": "fee1c5b5-482a-467b-9ec5-beed9c7b9e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 47s 6ms/step - loss: 0.3401 - acc: 0.8628 - val_loss: 0.3149 - val_acc: 0.8722\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.2328 - acc: 0.8961 - val_loss: 0.2118 - val_acc: 0.9044\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.1679 - acc: 0.9306 - val_loss: 0.1756 - val_acc: 0.9288\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.1274 - acc: 0.9525 - val_loss: 0.1292 - val_acc: 0.9493\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.1017 - acc: 0.9632 - val_loss: 0.1161 - val_acc: 0.9540\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0801 - acc: 0.9708 - val_loss: 0.1106 - val_acc: 0.9562\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0653 - acc: 0.9759 - val_loss: 0.1052 - val_acc: 0.9612\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0647 - acc: 0.9770 - val_loss: 0.1008 - val_acc: 0.9672\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0494 - acc: 0.9816 - val_loss: 0.1554 - val_acc: 0.9529\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0516 - acc: 0.9802 - val_loss: 0.1073 - val_acc: 0.9642\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0466 - acc: 0.9823 - val_loss: 0.0898 - val_acc: 0.9687\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0449 - acc: 0.9820 - val_loss: 0.0831 - val_acc: 0.9756\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0444 - acc: 0.9829 - val_loss: 0.0865 - val_acc: 0.9690\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0419 - acc: 0.9835 - val_loss: 0.0989 - val_acc: 0.9684\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0467 - acc: 0.9832 - val_loss: 0.0985 - val_acc: 0.9681\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0443 - acc: 0.9844 - val_loss: 0.0923 - val_acc: 0.9714\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0470 - acc: 0.9827 - val_loss: 0.1032 - val_acc: 0.9675\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0423 - acc: 0.9836 - val_loss: 0.0787 - val_acc: 0.9728\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0412 - acc: 0.9831 - val_loss: 0.0804 - val_acc: 0.9699\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0411 - acc: 0.9828 - val_loss: 0.0803 - val_acc: 0.9726\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0393 - acc: 0.9832 - val_loss: 0.1046 - val_acc: 0.9693\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0384 - acc: 0.9846 - val_loss: 0.0934 - val_acc: 0.9704\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0401 - acc: 0.9851 - val_loss: 0.1119 - val_acc: 0.9693\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0360 - acc: 0.9846 - val_loss: 0.0837 - val_acc: 0.9764\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.0387 - acc: 0.9840 - val_loss: 0.0899 - val_acc: 0.9737\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0365 - acc: 0.9845 - val_loss: 0.0940 - val_acc: 0.9665\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0366 - acc: 0.9849 - val_loss: 0.1040 - val_acc: 0.9718\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0362 - acc: 0.9853 - val_loss: 0.1182 - val_acc: 0.9647\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0371 - acc: 0.9849 - val_loss: 0.0832 - val_acc: 0.9703\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.0334 - acc: 0.9852 - val_loss: 0.0955 - val_acc: 0.9709\n",
      "CPU times: user 21min 25s, sys: 19.7 s, total: 21min 45s\n",
      "Wall time: 21min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training the model\n",
    "history4 = model4.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size= 64,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Q-Ova3k8rUWY",
    "outputId": "d6cc0412-d0f6-4975-ff6d-f028fec42333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 4s 1ms/step\n",
      "[0.09553245016018255, 0.9708743404444088]\n"
     ]
    }
   ],
   "source": [
    "score4 = model4.evaluate(X_test, Y_test)\n",
    "print(score4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "YlwKzjjnrUUS",
    "outputId": "efa67cd8-280f-49b3-901f-a405f1065c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 535        0  ...                   0                 2\n",
      "SITTING                  3      414  ...                   0                 2\n",
      "STANDING                 0       80  ...                   0                 0\n",
      "WALKING                  0        0  ...                  25                12\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 418                 1\n",
      "WALKING_UPSTAIRS         0        0  ...                   5               466\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model5:: Architecture of LSTM(120) + Dropout(0.2) + l2 reg + rmsprop + sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "fS0VFPeHb8XG",
    "outputId": "fcdda984-74c5-4243-ca68-a33597952c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128, 120)          62400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 120)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 120)               115680    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 726       \n",
      "=================================================================\n",
      "Total params: 178,806\n",
      "Trainable params: 178,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# With One LSTM Layer Model 1  # \n",
    "from keras import regularizers\n",
    "n_hidden = 120\n",
    "\n",
    "model5 = Sequential()\n",
    "\n",
    "# 1 LSTM layer\n",
    "model5.add(LSTM(n_hidden, input_shape = (timesteps, input_dim),kernel_regularizer=regularizers.l2(0.001), return_sequences = True)) # \n",
    "model5.add(Dropout(0.2))\n",
    "model5.add(LSTM(n_hidden))\n",
    "model5.add(Dense(n_classes, activation = 'sigmoid'))\n",
    "\n",
    "model5.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "print(model5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hspkRB2tb8St",
    "outputId": "44cfd00f-c2b7-4a55-c283-cde7edf8af1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.3401 - acc: 0.8633 - val_loss: 0.3031 - val_acc: 0.8701\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.2371 - acc: 0.8936 - val_loss: 0.2034 - val_acc: 0.9147\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.1735 - acc: 0.9275 - val_loss: 0.1988 - val_acc: 0.9175\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.1435 - acc: 0.9455 - val_loss: 0.1584 - val_acc: 0.9384\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.1283 - acc: 0.9526 - val_loss: 0.1741 - val_acc: 0.9325\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.1031 - acc: 0.9649 - val_loss: 0.1710 - val_acc: 0.9416\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0879 - acc: 0.9700 - val_loss: 0.1748 - val_acc: 0.9423\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0781 - acc: 0.9739 - val_loss: 0.1208 - val_acc: 0.9598\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0643 - acc: 0.9782 - val_loss: 0.2129 - val_acc: 0.9399\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0662 - acc: 0.9771 - val_loss: 0.2555 - val_acc: 0.9295\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0601 - acc: 0.9795 - val_loss: 0.1097 - val_acc: 0.9628\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0548 - acc: 0.9810 - val_loss: 0.1137 - val_acc: 0.9641\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0543 - acc: 0.9807 - val_loss: 0.1084 - val_acc: 0.9643\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0523 - acc: 0.9810 - val_loss: 0.1178 - val_acc: 0.9640\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0566 - acc: 0.9806 - val_loss: 0.0952 - val_acc: 0.9683\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0532 - acc: 0.9813 - val_loss: 0.1558 - val_acc: 0.9587\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0535 - acc: 0.9807 - val_loss: 0.1690 - val_acc: 0.9567\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0504 - acc: 0.9819 - val_loss: 0.1125 - val_acc: 0.9676\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0510 - acc: 0.9821 - val_loss: 0.1061 - val_acc: 0.9696\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0545 - acc: 0.9815 - val_loss: 0.1068 - val_acc: 0.9661\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0485 - acc: 0.9832 - val_loss: 0.0999 - val_acc: 0.9738\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0482 - acc: 0.9829 - val_loss: 0.1025 - val_acc: 0.9691\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0515 - acc: 0.9825 - val_loss: 0.1023 - val_acc: 0.9683\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.0480 - acc: 0.9835 - val_loss: 0.1004 - val_acc: 0.9680\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0460 - acc: 0.9836 - val_loss: 0.0954 - val_acc: 0.9720\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0446 - acc: 0.9836 - val_loss: 0.1036 - val_acc: 0.9695\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0436 - acc: 0.9843 - val_loss: 0.1164 - val_acc: 0.9654\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.0443 - acc: 0.9842 - val_loss: 0.0846 - val_acc: 0.9721\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.0426 - acc: 0.9830 - val_loss: 0.0901 - val_acc: 0.9746\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 62s 8ms/step - loss: 0.0439 - acc: 0.9830 - val_loss: 0.0927 - val_acc: 0.9678\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history5 = model5.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size= 64,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "06ci-A9fJEf4",
    "outputId": "7379afaf-5f59-492a-d57b-5a8545a139a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 5s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09267068984828229, 0.9678203892133419]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score5 = model5.evaluate(X_test, Y_test)\n",
    "score5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "6d0rrUciJEoF",
    "outputId": "0aec7aa1-bdaf-49e9-cf9a-31491cd2b6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 502       35  ...                   0                 0\n",
      "SITTING                  2      417  ...                   0                 6\n",
      "STANDING                 0      112  ...                   0                 3\n",
      "WALKING                  0        3  ...                  14                12\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 415                 0\n",
      "WALKING_UPSTAIRS         0        0  ...                   2               441\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model5.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AVfUb9iOYSS"
   },
   "outputs": [],
   "source": [
    "# utility function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4\n",
    "# https://stackoverflow.com/a/14434334\n",
    "# this function is used to update the plots for each epoch and error\n",
    "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
    "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "qucPs79uPWaj",
    "outputId": "32c58e5d-efce-4e70-c930-763525f8ed89"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEHCAYAAABbZ7oVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njuM4heJKwnEcxymU/w/m7xQMXQaFLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch') ; ax.set_ylabel('binary_crossentropy')\n",
    "\n",
    "# list of epoch numbers\n",
    "x = list(range(1,epochs+1))\n",
    "\n",
    "# print(history.history.keys())\n",
    "# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
    "# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "# we will get val_loss and val_acc only when you pass the paramter validation_data\n",
    "# val_loss : validation loss\n",
    "# val_acc : validation accuracy\n",
    "\n",
    "# loss : training loss\n",
    "# acc : train accuracy\n",
    "# for each key in histrory.histrory we will have a list of length equal to number of epochs\n",
    "\n",
    "vy = history5.history['val_loss']\n",
    "ty = history5.history['loss']\n",
    "plt_dynamic(x, vy, ty, ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model6 :: Architecture of LSTM(120) + Dropout(0.7) + l2 reg + rmsprop + sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sPmW2nCNYdCV",
    "outputId": "1c4f4c5b-e3fd-483b-cc2d-a335478d8c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128, 120)          62400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 120)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 120)               115680    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 726       \n",
      "=================================================================\n",
      "Total params: 178,806\n",
      "Trainable params: 178,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.3705 - acc: 0.8606 - val_loss: 0.3305 - val_acc: 0.8720\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.3343 - acc: 0.8709 - val_loss: 0.3576 - val_acc: 0.8618\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.2538 - acc: 0.8903 - val_loss: 0.2771 - val_acc: 0.8854\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.1986 - acc: 0.9169 - val_loss: 0.1806 - val_acc: 0.9353\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.1488 - acc: 0.9571 - val_loss: 0.1084 - val_acc: 0.9636\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.1200 - acc: 0.9652 - val_loss: 0.2781 - val_acc: 0.9151\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.1101 - acc: 0.9660 - val_loss: 0.1660 - val_acc: 0.9533\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.1410 - acc: 0.9622 - val_loss: 0.1025 - val_acc: 0.9712\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.1038 - acc: 0.9691 - val_loss: 0.1108 - val_acc: 0.9683\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.0915 - acc: 0.9734 - val_loss: 0.1271 - val_acc: 0.9589\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.0904 - acc: 0.9723 - val_loss: 0.0979 - val_acc: 0.9703\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.0963 - acc: 0.9719 - val_loss: 0.0830 - val_acc: 0.9731\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.0795 - acc: 0.9767 - val_loss: 0.0922 - val_acc: 0.9743\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.0726 - acc: 0.9784 - val_loss: 0.1042 - val_acc: 0.9697\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.0791 - acc: 0.9774 - val_loss: 0.0968 - val_acc: 0.9701\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.0798 - acc: 0.9771 - val_loss: 0.0951 - val_acc: 0.9690\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.0742 - acc: 0.9766 - val_loss: 0.0945 - val_acc: 0.9713\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.0748 - acc: 0.9772 - val_loss: 0.1719 - val_acc: 0.9295\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.0733 - acc: 0.9766 - val_loss: 0.1109 - val_acc: 0.9661\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.0780 - acc: 0.9779 - val_loss: 0.0917 - val_acc: 0.9712\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.0680 - acc: 0.9796 - val_loss: 0.1131 - val_acc: 0.9679\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.0696 - acc: 0.9781 - val_loss: 0.1164 - val_acc: 0.9578\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.0703 - acc: 0.9792 - val_loss: 0.1089 - val_acc: 0.9723\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 121s 17ms/step - loss: 0.0783 - acc: 0.9784 - val_loss: 0.1086 - val_acc: 0.9683\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.0696 - acc: 0.9783 - val_loss: 0.0863 - val_acc: 0.9719\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.0840 - acc: 0.9764 - val_loss: 0.0776 - val_acc: 0.9740\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.1573 - acc: 0.9591 - val_loss: 0.0823 - val_acc: 0.9735\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.0718 - acc: 0.9787 - val_loss: 0.0821 - val_acc: 0.9731\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.0931 - acc: 0.9712 - val_loss: 0.0855 - val_acc: 0.9746\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.0705 - acc: 0.9794 - val_loss: 0.0985 - val_acc: 0.9742\n"
     ]
    }
   ],
   "source": [
    "# With One LSTM Layer Model 1  # \n",
    "n_hidden = 120\n",
    "# Initiliazing the sequential model\n",
    "model6 = Sequential()\n",
    "\n",
    "# Configuring the parameters\n",
    "model6.add(LSTM(n_hidden, input_shape = (timesteps, input_dim),kernel_regularizer=regularizers.l2(0.01), return_sequences = True)) # \n",
    "model6.add(Dropout(0.7)) # Adding a dropout layer\n",
    "model6.add(LSTM(n_hidden)) # Configuring the parameters\n",
    "model6.add(Dropout(0.7)) # Adding a dropout layer\n",
    "model6.add(Dense(n_classes, activation = 'sigmoid')) # Adding a dense output layer with sigmoid activation\n",
    "# Compiling the model\n",
    "model6.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "print(model6.summary())\n",
    "# Training the model\n",
    "history6 = model6.fit(X_train,Y_train,batch_size=batch_size,validation_data=(X_test, Y_test),epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gi48Wq8lYkEE",
    "outputId": "91badfa9-11c6-47d4-8e33-bac5a3ff61d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 6s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09851918673862693, 0.9741545128021396]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score6 = model6.evaluate(X_test, Y_test)\n",
    "score6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "IDAizedBmLZG",
    "outputId": "31b29088-e3e2-4c72-d965-2d560b64d5f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 537        0  ...                   0                 0\n",
      "SITTING                  1      376  ...                   0                 0\n",
      "STANDING                 0       74  ...                   0                 0\n",
      "WALKING                  0        0  ...                   8                16\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 417                 3\n",
      "WALKING_UPSTAIRS         0        0  ...                  16               455\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion MAtrix\n",
    "print(confusion_matrix(Y_test, model6.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "W937dN_PmcvV",
    "outputId": "5f5fe7bf-e4bf-41d9-cbf6-10cfedbc01bd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkIJhL2QKm+wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epochs');ax.set_ylabel('binary crossentropy loss');ax.set_title(\"Model 6 loss vs epochs\")\n",
    "x = list(range(1,31))\n",
    "\n",
    "vy = history6.history['val_loss']\n",
    "ty = history6.history['loss']\n",
    "\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eykkPVURaGCi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------+--------+------------+------------+-----------+---------+----------+-----------+\n",
      "|          Model          | Hidden layer | epochs | batch_size | activation | Optimizer | Dropout | Val_Loss | Val_acc % |\n",
      "+-------------------------+--------------+--------+------------+------------+-----------+---------+----------+-----------+\n",
      "|        LSTM Model       |      32      |   30   |     16     |  Sigmoid   |  rmsprop  |   0.5   |  0.5088  |   0.8958  |\n",
      "|       LSTM Model1       |      32      |   30   |     16     |  Sigmoid   |  rmsprop  |   0.5   |  0.3348  |   0.9155  |\n",
      "|       LSTM Model2       |      42      |   30   |     16     |  Sigmoid   |  rmsprop  |   0.5   |  0.8907  |   0.8493  |\n",
      "|       LSTM Model3       |      80      |   30   |     64     |  Sigmoid   |    adam   |   0.25  |  0.1164  |   0.9629  |\n",
      "|       LSTM Model4       |      80      |   30   |     64     |  Sigmoid   |  rmsprop  |   0.25  |  0.0955  |   0.9708  |\n",
      "| LSTM Model5 with l2 Reg |     120      |   30   |     64     |  Sigmoid   |  rmsprop  |   0.2   |  0.0926  |   0.9678  |\n",
      "| LSTM Model6 with l2 Reg |     120      |   30   |     16     |  Sigmoid   |  rmsprop  |   0.7   |  0.0985  |   0.9741  |\n",
      "+-------------------------+--------------+--------+------------+------------+-----------+---------+----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "p = PrettyTable()\n",
    "p.field_names = [\"Model\",\"Hidden layer\",\"epochs\", \"batch_size\",\"activation\",\"Optimizer\",\"Dropout\", \"Val_Loss\", \"Val_acc %\"]\n",
    "p.add_row(['LSTM Model',  32, 30, 16, 'Sigmoid', 'rmsprop', 0.5, 0.5088, 0.8958])\n",
    "p.add_row(['LSTM Model1', 32, 30, 16, 'Sigmoid', 'rmsprop', 0.5, 0.3348, 0.9155])\n",
    "p.add_row(['LSTM Model2', 42, 30, 16, 'Sigmoid', 'rmsprop', 0.5, 0.8907, 0.8493])\n",
    "p.add_row(['LSTM Model3', 80, 30, 64, 'Sigmoid', 'adam',   0.25, 0.1164, 0.9629])\n",
    "p.add_row(['LSTM Model4', 80, 30, 64, 'Sigmoid', 'rmsprop', 0.25, 0.0955, 0.9708])\n",
    "p.add_row(['LSTM Model5 with l2 Reg', 120, 30, 64, 'Sigmoid', 'rmsprop', 0.2, 0.0926, 0.9678])\n",
    "p.add_row(['LSTM Model6 with l2 Reg', 120, 30, 16, 'Sigmoid', 'rmsprop', 0.7, 0.0985, 0.9741])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations ::\n",
    "\n",
    "- By increasing the hidden layers to 80 we got a very good result of 0.9629 % with \"adam\" optimzier .\n",
    "- Lstm + dropout(0.25) model ,with 80 hidden layers with \"rmsprop\" optimizer we got a test acccuracy of 0.9708 %.\n",
    "- Lstm + Large dropout(0.7) model ,with 120 hidden layers with \"sigmoid\" activation function we got a test acccuracy of 0.9741%.\n",
    "\n",
    "\n",
    "- Cite : https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b\n",
    "- Cite : https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8f_LXQ3oCYv"
   },
   "source": [
    "<img src='hartq.png'>\n",
    "\n",
    "<nav style=\"text-align:center\">\n",
    "<a href=\"https://www.linkedin.com/in/rameshbattuai/\">**Sign Off RAMESH BATTU**</a></nav>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Human activity detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
